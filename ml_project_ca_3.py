# -*- coding: utf-8 -*-
"""ML_Project_CA_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10NCRkJlzxKPJFmXAWs2DoLG-oJHf8wPb

# ğŸ§  Emotion Detection from Facial Expressions using CNN & MobileNetV2

## ğŸ“˜ Project Overview
This project focuses on **automatic emotion recognition** from human facial expressions using **Deep Learning (DL)** and **Machine Learning (ML)** techniques.  
It uses **Convolutional Neural Networks (CNN)** and **MobileNetV2**, along with traditional ML models like **Support Vector Machine (SVM)** and **Random Forest (RF)** for comparative analysis.  

The goal is to classify images into emotion categories such as:
ğŸ˜„ Happy | ğŸ˜¢ Sad | ğŸ˜  Angry | ğŸ˜® Surprise | ğŸ˜ Neutral | ğŸ˜¨ Fear | ğŸ˜– Disgust

---

## ğŸ¯ Objectives
- Detect and classify human emotions from facial images  
- Compare the performance of DL (CNN, MobileNetV2) and ML (SVM, Random Forest) models  
- Evaluate models using metrics such as **Accuracy, Loss, Precision, Recall, and F1-score**  
- Visualize and interpret results effectively  

---

## ğŸ§© Methodology
1. **Data Preprocessing:**  
   - Collected labeled facial expression images  
   - Resized and normalized images (224Ã—224)  
   - Data augmentation applied for better generalization  

2. **Model Implementation:**  
   - ğŸ§  **Custom CNN Model** built from scratch  
   - âš¡ **MobileNetV2** used as a transfer learning model  
   - Extracted features from CNN for ML model training (SVM & RF)  

3. **Evaluation:**  
   - Compared accuracy and loss across all models  
   - Generated classification reports for ML models  

4. **Prediction:**  
   - Uploaded facial images and predicted emotions using trained models  

---

## ğŸ“Š Performance Metrics
- **Accuracy (%)**
- **Loss**
- **F1-Score**

---

## ğŸ’» Technologies Used
| Category | Tools / Libraries |
|-----------|-------------------|
| Language | Python ğŸ |
| Deep Learning | TensorFlow, Keras |
| Machine Learning | Scikit-learn |
| Visualization | Matplotlib |
| Image Processing | OpenCV |

---

## ğŸ” Results Summary
| Model | Type | Key Observation |
|--------|------|-----------------|
| CNN | Deep Learning | Good baseline performance |
| MobileNetV2 | Transfer Learning | Improved accuracy and convergence |
| SVM | ML | Performs well with CNN features |
| Random Forest | ML | Provides balanced results but slightly lower accuracy |

---

## ğŸ“· Example Prediction
```text
Input: facial_image.jpg  
Predictions:  
CNN â†’ ğŸ˜Š Happy  
SVM â†’ ğŸ˜Š Happy  
Random Forest â†’ ğŸ˜ Neutral
"""

!pip install kaggle

"""âœ… Interpretation:

This command installs the Kaggle API ğŸ“¦ â€” letting you access Kaggle datasets, competitions & notebooks directly from Python ğŸ’».
"""

from google.colab import files
files.upload()

"""âœ… Interpretation:

This code opens a file upload dialog in Google Colab, letting you choose files from your computer to upload into the notebookâ€™s workspace ğŸ’»â¬†ï¸.
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""âœ… Interpretation:

These commands set up your Kaggle API credentials:

ğŸ“ !mkdir -p ~/.kaggle â†’ Creates a hidden folder for Kaggle config files.

ğŸ“„ !cp kaggle.json ~/.kaggle/ â†’ Copies your kaggle.json API key file there.

ğŸ”’ !chmod 600 ~/.kaggle/kaggle.json â†’ Secures the file by restricting access permissions.

"""

!kaggle datasets download -d msambare/fer2013
!unzip fer2013.zip -d fer2013_data

"""ğŸ“¦ Interpretation:


â¬‡ï¸ !kaggle datasets download -d msambare/fer2013 â†’ Downloads the FER2013 facial emotion dataset from Kaggle.


ğŸ“‚ !unzip fer2013.zip -d fer2013_data â†’ Extracts the downloaded ZIP file into a folder named fer2013_data for use.


"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix

"""ğŸ§  Interpretation:

These imports bring in essential libraries for data analysis, image processing, and deep learning:


ğŸ§® numpy, pandas â†’ Handle numerical & tabular data.


ğŸ“Š matplotlib, seaborn â†’ For plotting and visualizing data.


ğŸ“¸ cv2 â†’ Image processing with OpenCV.


ğŸ¤– tensorflow & keras â†’ Build and train deep learning models.


ğŸ§± Sequential, Conv2D, etc. â†’ Layers for CNN model design.


ğŸŒ€ ImageDataGenerator â†’ Augments images for better training.


âš¡ MobileNetV2 â†’ Pretrained model for feature extraction or transfer learning.


ğŸ§® Adam â†’ Optimizer for model training.


ğŸ“ˆ classification_report, confusion_matrix â†’ Evaluate model performance.


"""

train_dir = "fer2013/train"
test_dir = "fer2013/test"
img_size = (48,48)

train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, zoom_range=0.2,
                                   width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(train_dir, target_size=img_size, color_mode='grayscale',
                                               batch_size=64, class_mode='categorical')
test_data = test_datagen.flow_from_directory(test_dir, target_size=img_size, color_mode='grayscale',
                                             batch_size=64, class_mode='categorical')

class_labels = list(train_data.class_indices.keys())
print("Classes:", class_labels)

"""ğŸ§  Interpretation:

Loads and preprocesses FER2013 images ğŸ–¼ï¸ â€” resizing to 48Ã—48, normalizing pixels, applying augmentations for training, and preparing labeled batches for model training & testing ğŸ“¦.
"""

cnn_model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    Flatten(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(7, activation='softmax')
])

cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
cnn_model.summary()

"""ğŸ§  Interpretation:

Builds a CNN model for emotion detection ğŸ˜ŠğŸ˜¡ğŸ˜¢:

ğŸ§± Layers: 3 Conv2D + Pooling â†’ Flatten â†’ Dense.

âš™ï¸ Uses ReLU activations, BatchNorm, and Dropout (to prevent overfitting).

ğŸ¯ Output: 7 neurons (for 7 emotions) with softmax.

ğŸ§® Compiled with Adam optimizer & categorical crossentropy loss
"""

cnn_history = cnn_model.fit(train_data, validation_data=test_data, epochs=10)

"""ğŸš€ Interpretation:


Trains the CNN model ğŸ§  on the FER2013 dataset for 10 epochs, using training data for learning and test data for validation ğŸ“Š â€” tracking accuracy & loss over time.
"""

base_model = MobileNetV2(input_shape=(48,48,3), include_top=False, weights='imagenet')
base_model.trainable = False

mobilenet_model = Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dropout(0.4),
    Dense(7, activation='softmax')
])

mobilenet_model.compile(optimizer=Adam(0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
mobilenet_model.summary()

"""ğŸ¤– Interpretation:


Builds a transfer learning model using MobileNetV2 pretrained on ImageNet ğŸ§ :

ğŸ§± Freezes base layers (trainable=False) to keep learned features.

Adds pooling, dense, and dropout layers for emotion classification (7 classes).

âš™ï¸ Compiled with Adam (lr=0.0001) and categorical crossentropy for training.
"""

mobilenet_history = mobilenet_model.fit(train_data, validation_data=test_data, epochs=10)

"""ğŸš€ Interpretation:


Trains the MobileNetV2-based model ğŸ§  on the FER2013 dataset for 10 epochs, using training data for learning and test data for validation ğŸ“Š â€” tracking accuracy and loss during training.
"""

from sklearn.metrics import classification_report
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dropout, Flatten, Dense,
                                     BatchNormalization, GlobalAveragePooling2D, Input)
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.optimizers import Adam

# Define paths, image size, and batch size (assuming dataset is downloaded and unzipped)
test_dir = "/content/fer2013/test"
IMG_SIZE = (48, 48)
BATCH = 64

# Define data generators
test_datagen = ImageDataGenerator(rescale=1./255)

test_gen = test_datagen.flow_from_directory(test_dir,
                                            target_size=IMG_SIZE,
                                            color_mode='grayscale',
                                            batch_size=BATCH,
                                            class_mode='categorical',
                                            shuffle=False)

# Create a new generator for MobileNetV2 with RGB color mode
test_datagen_rgb = ImageDataGenerator(rescale=1./255)
test_gen_rgb = test_datagen_rgb.flow_from_directory(test_dir, target_size=IMG_SIZE, color_mode='rgb',
                                             batch_size=BATCH, class_mode='categorical', shuffle=False)

# Build and Train the CNN model (if not already trained)
try:
    cnn_model
except NameError:
    print("CNN model not found. Building and training a placeholder CNN model...")
    # Build a placeholder CNN model for demonstration if the actual model is not available
    def build_custom_cnn(input_shape=(48,48,1), n_classes=7):
        inp = Input(shape=input_shape)
        x = Conv2D(32, (3,3), activation='relu', padding='same')(inp)
        x = BatchNormalization()(x)
        x = MaxPooling2D((2,2))(x)
        x = GlobalAveragePooling2D()(x)
        out = Dense(n_classes, activation='softmax')(x)
        model = Model(inputs=inp, outputs=out)
        return model
    cnn_model = build_custom_cnn(input_shape=(48,48,1), n_classes=len(test_gen.class_indices))
    cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    # Note: This placeholder model is not trained. For proper evaluation, train the actual model first.


# Build and Train the MobileNetV2 model (if not already trained)
try:
    mobilenet_model
except NameError:
    print("MobileNetV2 model not found. Building and training a placeholder MobileNetV2 model...")
    # Build a placeholder MobileNetV2 model for demonstration if the actual model is not available
    base = MobileNetV2(input_shape=(48,48,3), include_top=False, weights='imagenet')
    base.trainable = False
    mobilenet_model = Sequential([
        base,
        tf.keras.layers.GlobalAveragePooling2D(),
        Dense(len(test_gen_rgb.class_indices), activation='softmax')
    ])
    mobilenet_model.compile(optimizer=Adam(0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
    # Note: This placeholder model is not trained. For proper evaluation, train the actual model first.


# Get true labels from the original grayscale test data generator (labels are the same)
y_true = test_gen.classes

# Get predictions from CNN model (using grayscale generator)
Y_pred_cnn = cnn_model.predict(test_gen)
y_pred_cnn = np.argmax(Y_pred_cnn, axis=1)

# Get predictions from MobileNet model (using RGB generator)
Y_pred_mobilenet = mobilenet_model.predict(test_gen_rgb)
y_pred_mobilenet = np.argmax(Y_pred_mobilenet, axis=1)


# Classification report for CNN
report_cnn = classification_report(y_true, y_pred_cnn, target_names=list(test_gen.class_indices.keys()), output_dict=True)
report_df_cnn = pd.DataFrame(report_cnn).transpose()

# Plot F1-scores for CNN
plt.figure(figsize=(10,6))
# Exclude 'accuracy', 'macro avg', 'weighted avg' from plotting
sns.barplot(x=report_df_cnn.index[:-3], y=report_df_cnn["f1-score"][:-3], palette="coolwarm")
plt.title("F1-Score per Emotion - CNN Model")
plt.ylabel("F1-Score")
plt.xticks(rotation=45)
plt.show()

# Classification report for MobileNet
report_mobilenet = classification_report(y_true, y_pred_mobilenet, target_names=list(test_gen_rgb.class_indices.keys()), output_dict=True)
report_df_mobilenet = pd.DataFrame(report_mobilenet).transpose()

# Plot F1-scores for MobileNet
plt.figure(figsize=(10,6))
# Exclude 'accuracy', 'macro avg', 'weighted avg' from plotting
sns.barplot(x=report_df_mobilenet.index[:-3], y=report_df_mobilenet["f1-score"][:-3], palette="crest")
plt.title("F1-Score per Emotion - MobileNetV2 Model")
plt.ylabel("F1-Score")
plt.xticks(rotation=45)
plt.show()

"""ğŸ“Š Interpretation:


This code evaluates and compares CNN ğŸ§  and MobileNetV2 ğŸ¤– models on the FER2013 test dataset to measure emotion recognition performance.

Steps:

ğŸ“‚ Loads test images (grayscale for CNN, RGB for MobileNetV2).

âš™ï¸ Builds placeholder models if originals arenâ€™t available.

ğŸ” Makes predictions using both models.

ğŸ“ˆ Generates classification reports (precision, recall, F1-score).

ğŸ¨ Visualizes F1-scores per emotion for both models using bar charts â€” helping compare their accuracy by emotion category.
"""

Y_pred = cnn_model.predict(test_data)
y_pred = np.argmax(Y_pred, axis=1)
print('Confusion Matrix:\n')
cm = confusion_matrix(test_data.classes, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.show()

"""ğŸ“Š Interpretation:


Generates a confusion matrix for the CNN model ğŸ§  to evaluate performance:


ğŸ” Y_pred â†’ Modelâ€™s predicted probabilities.


ğŸ§® y_pred â†’ Converts them to class labels.


ğŸ“ˆ confusion_matrix â†’ Compares true vs. predicted emotions.


ğŸ¨ sns.heatmap â†’ Visualizes it as a blue heatmap, showing which emotions were correctly or wrongly classified.


"""

img_rgb = image.load_img(img_path, color_mode='rgb', target_size=(48,48))
img_rgb_array = image.img_to_array(img_rgb)
img_rgb_array = np.expand_dims(img_rgb_array, axis=0)
img_rgb_array = img_rgb_array / 255.0
mobilenet_pred = mobilenet_model.predict(img_rgb_array)

"""ğŸ–¼ï¸ Interpretation:

Prepares a single image for prediction using MobileNetV2 ğŸ¤–:

ğŸ“¸ Loads and resizes the image to 48Ã—48 (RGB).

ğŸ”¢ Converts it to an array and adds a batch dimension.

âš™ï¸ Normalizes pixel values (0â€“1).

ğŸ§  mobilenet_model.predict() â†’ Predicts the emotion class for that image.
"""

# Run only if you need to download from Kaggle
!pip install -q kaggle
from google.colab import files
print("Upload your kaggle.json now")
files.upload()   # upload kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d msambare/fer2013
!unzip -q fer2013.zip -d fer2013
# Some versions extract train/test folders; check with:
!ls -la fer2013 | sed -n '1,120p'

"""âš™ï¸ Interpretation:


This setup downloads the FER2013 dataset from Kaggle into Google Colab ğŸ“¦:

ğŸ“¥ Installs the Kaggle API.

ğŸ”‘ Prompts you to upload your kaggle.json (API key).

ğŸ—‚ï¸ Creates a hidden Kaggle folder and secures credentials.

â¬‡ï¸ Downloads and unzips the FER2013 dataset into fer2013/.

ğŸ§¾ Lists the extracted files/folders to confirm successful download.
"""

# core libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
from glob import glob
import seaborn as sns

# tensorflow / keras
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dropout, Flatten, Dense,
                                     BatchNormalization, GlobalAveragePooling2D, Input)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.optimizers import Adam

# sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""ğŸ§  Interpretation:

This code imports all the essential libraries for a deep learning + machine learning emotion detection project ğŸ˜„ğŸ˜ ğŸ˜¢:


ğŸ§® NumPy, Pandas â†’ Data handling & manipulation.


ğŸ“Š Matplotlib, Seaborn â†’ Visualization & plotting.


ğŸ“¸ cv2 (OpenCV) â†’ Image processing.


ğŸ¤– TensorFlow/Keras â†’ Building and training CNN & transfer learning models.


âš™ï¸ MobileNetV2 â†’ Pretrained model for transfer learning.


ğŸ§± Layers & Model API â†’ For CNN architecture design.


ğŸŒ² RandomForestClassifier, SVC â†’ Traditional ML classifiers (for comparison).


ğŸ“ˆ Metrics â†’ Evaluate model performance (accuracy, confusion matrix, report).


"""

# paths (update if different)
train_dir = "/content/fer2013/train"   # each class folder inside
test_dir  = "/content/fer2013/test"

# image size and batch
IMG_SIZE = (48,48)
BATCH = 64

# generators
train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=20,
                                   width_shift_range=0.1,
                                   height_shift_range=0.1,
                                   zoom_range=0.1,
                                   horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

train_gen = train_datagen.flow_from_directory(train_dir,
                                              target_size=IMG_SIZE,
                                              color_mode='grayscale',
                                              batch_size=BATCH,
                                              class_mode='categorical',
                                              shuffle=True)

test_gen = test_datagen.flow_from_directory(test_dir,
                                            target_size=IMG_SIZE,
                                            color_mode='grayscale',
                                            batch_size=BATCH,
                                            class_mode='categorical',
                                            shuffle=False)

class_labels = list(train_gen.class_indices.keys())
print("Classes:", class_labels)

"""ğŸ“¦ Interpretation:

Prepares the FER2013 dataset for training and testing ğŸ§ :

ğŸ“ Defines train/test folder paths.

ğŸ–¼ï¸ Resizes all images to 48Ã—48 and normalizes pixel values.

ğŸ”„ Applies data augmentation (rotation, shift, zoom, flip) to boost training diversity.

ğŸ§® Loads grayscale images in batches of 64 with emotion labels.

ğŸ·ï¸ Prints detected emotion classes (e.g., happy, sad, angry).
"""

def build_custom_cnn(input_shape=(48,48,1), n_classes=7):
    inp = Input(shape=input_shape)
    x = Conv2D(32, (3,3), activation='relu', padding='same')(inp)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = GlobalAveragePooling2D(name='feature_layer')(x)   # named feature layer
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    out = Dense(n_classes, activation='softmax')(x)

    model = Model(inputs=inp, outputs=out)
    return model

cnn_model = build_custom_cnn(input_shape=(48,48,1), n_classes=len(class_labels))
cnn_model.compile(optimizer=Adam(learning_rate=1e-3),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
cnn_model.summary()

"""ğŸ§  Interpretation:


Builds a custom CNN model for emotion classification on FER2013 ğŸ˜„ğŸ˜¢ğŸ˜ :


ğŸ§± Layers: 3 Conv2D + Pooling blocks â†’ GlobalAveragePooling â†’ Dense layers.


âš™ï¸ BatchNormalization for stability & Dropout (0.5) to reduce overfitting.


ğŸ·ï¸ Output: 7 neurons (one per emotion) with softmax activation.


ğŸ§® Compiled with Adam (lr=0.001) and categorical crossentropy for multi-class accuracy tracking.


"""

# quick demo: 5 epochs. Increase later to 25-50 as needed.
EPOCHS_CNN = 5
history_cnn = cnn_model.fit(train_gen, validation_data=test_gen, epochs=EPOCHS_CNN)

"""ğŸš€ Interpretation:

Trains the custom CNN model ğŸ§  for 5 epochs using training data and validates on the test set ğŸ“Š â€” tracking accuracy and loss to monitor learning progress.
"""

# MobileNet base expects 3 channels. We'll build a new generator for RGB.
train_gen_rgb = train_datagen.flow_from_directory(train_dir,
                                                  target_size=IMG_SIZE,
                                                  color_mode='rgb',
                                                  batch_size=BATCH,
                                                  class_mode='categorical',
                                                  shuffle=True)

test_gen_rgb = test_datagen.flow_from_directory(test_dir,
                                                target_size=IMG_SIZE,
                                                color_mode='rgb',
                                                batch_size=BATCH,
                                                class_mode='categorical',
                                                shuffle=False)

# Build MobileNetV2 model
base = MobileNetV2(input_shape=(48,48,3), include_top=False, weights='imagenet')
base.trainable = False  # freeze initially

inp = Input(shape=(48,48,3))
x = base(inp, training=False)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.4)(x)
x = Dense(128, activation='relu')(x)
out = Dense(len(class_labels), activation='softmax')(x)
mobilenet_model = Model(inputs=inp, outputs=out)
mobilenet_model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
mobilenet_model.summary()

# Train MobileNet quick demo
EPOCHS_MOB = 5
history_mob = mobilenet_model.fit(train_gen_rgb, validation_data=test_gen_rgb, epochs=EPOCHS_MOB)

"""ğŸ¤– Interpretation:


Builds and trains a MobileNetV2 transfer learning model for emotion recognition ğŸ˜„ğŸ˜¢ğŸ˜ :

ğŸ¨ Creates new RGB generators since MobileNetV2 needs 3-channel input.

ğŸ§± Uses pretrained ImageNet weights, freezes the base for feature extraction.

ğŸ”„ Adds pooling, dropout (0.4), and dense layers for classification (7 emotions).

âš™ï¸ Compiled with Adam (lr = 1e-4) and trained for 5 epochs to test performance.
"""

# core libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
from glob import glob
import seaborn as sns

# tensorflow / keras
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dropout, Flatten, Dense,
                                     BatchNormalization, GlobalAveragePooling2D, Input)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.optimizers import Adam

# sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# paths (update if different)
train_dir = "/content/fer2013/train"   # each class folder inside
test_dir  = "/content/fer2013/test"

# image size and batch
IMG_SIZE = (48,48)
BATCH = 64

# generators
train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=20,
                                   width_shift_range=0.1,
                                   height_shift_range=0.1,
                                   zoom_range=0.1,
                                   horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

train_gen = train_datagen.flow_from_directory(train_dir,
                                              target_size=IMG_SIZE,
                                              color_mode='grayscale',
                                              batch_size=BATCH,
                                              class_mode='categorical',
                                              shuffle=True)

test_gen = test_datagen.flow_from_directory(test_dir,
                                            target_size=IMG_SIZE,
                                            color_mode='grayscale',
                                            batch_size=BATCH,
                                            class_mode='categorical',
                                            shuffle=False)

class_labels = list(train_gen.class_indices.keys())
print("Classes:", class_labels)

# Build the CNN model
def build_custom_cnn(input_shape=(48,48,1), n_classes=7):
    inp = Input(shape=input_shape)
    x = Conv2D(32, (3,3), activation='relu', padding='same')(inp)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(64, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2,2))(x)

    x = GlobalAveragePooling2D(name='feature_layer')(x)   # named feature layer
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    out = Dense(n_classes, activation='softmax')(x)

    model = Model(inputs=inp, outputs=out)
    return model

cnn_model = build_custom_cnn(input_shape=(48,48,1), n_classes=len(class_labels))
cnn_model.compile(optimizer=Adam(learning_rate=1e-3),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

# Train the CNN model (quick demo: 5 epochs)
EPOCHS_CNN = 5
history_cnn = cnn_model.fit(train_gen, validation_data=test_gen, epochs=EPOCHS_CNN)


# Create extractor model that outputs 'feature_layer'
feature_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.get_layer('feature_layer').output)

# helper to get features and labels from a generator (one pass)
def extract_features_from_generator(gen, extractor):
    steps = int(np.ceil(gen.samples / gen.batch_size))
    features = []
    labels = []
    gen.reset()
    for i in range(steps):
        X_batch, y_batch = next(gen) # Use next() to get the next batch
        feats = extractor.predict(X_batch, verbose=0) # Set verbose to 0
        features.append(feats)
        labels.append(np.argmax(y_batch, axis=1))
    X = np.vstack(features)
    y = np.hstack(labels)
    return X, y

# Extract for train (use grayscale generator for extractor)
X_train_feat, y_train = extract_features_from_generator(train_gen, feature_extractor)
X_test_feat, y_test   = extract_features_from_generator(test_gen, feature_extractor)
print("Train features:", X_train_feat.shape, "Test features:", X_test_feat.shape)

"""ğŸ§  Interpretation:


This code combines deep learning (CNN) and traditional ML to extract and reuse features for emotion detection ğŸ˜„ğŸ˜¢ğŸ˜ :

âš™ï¸ Imports all key libraries (TensorFlow, OpenCV, sklearn, etc.).

ğŸ“‚ Loads the FER2013 train/test images (grayscale, 48Ã—48).

ğŸ”„ Applies data augmentation for better model generalization.

ğŸ§± Builds a custom CNN with convolution, pooling, batch normalization, dropout, and dense layers.

ğŸš€ Trains it briefly for 5 epochs to learn emotion features.

ğŸ” Creates a feature extractor model that outputs embeddings from the feature_layer.

ğŸ“Š Extracts deep feature vectors (CNN outputs) for both train and test sets â€” turning images into numeric data (X_train_feat, X_test_feat) that can be used by machine learning models like SVM or Random Forest later.
"""

# SVM
svm_model = SVC(kernel='rbf', probability=True)
svm_model.fit(X_train_feat, y_train)
svm_pred = svm_model.predict(X_test_feat)
print("SVM accuracy:", accuracy_score(y_test, svm_pred))

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train_feat, y_train)
rf_pred = rf_model.predict(X_test_feat)
print("Random Forest accuracy:", accuracy_score(y_test, rf_pred))

"""ğŸ¤– Interpretation:


Uses CNN-extracted features ğŸ¯ to train and evaluate machine learning classifiers:

âš™ï¸ SVM (Support Vector Machine): Trains with RBF kernel to classify emotions; prints test accuracy.

ğŸŒ² Random Forest: Trains with 100 trees to predict emotions; prints test accuracy.

ğŸ“ˆ Both models use deep CNN features instead of raw pixels â€” combining deep learning feature extraction with traditional ML classification for improved performance.
"""

cnn_model.save("emotion_cnn_model.h5")
mobilenet_model.save("emotion_mobilenet_model.h5")

# sklearn models using joblib
import joblib
joblib.dump(svm_model, "svm_model.pkl")
joblib.dump(rf_model, "rf_model.pkl")

"""ğŸ’¾ Interpretation:


Saves all trained models for future use ğŸ”:

ğŸ§  emotion_cnn_model.h5 â†’ Saved CNN model.

ğŸ¤– emotion_mobilenet_model.h5 â†’ Saved MobileNetV2 model.

âš™ï¸ svm_model.pkl, rf_model.pkl â†’ Saved SVM and Random Forest models using joblib for reuse without retraining.
"""

from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import tensorflow as tf

# Assuming train_data and test_data generators are already defined

class_weights = compute_class_weight('balanced', classes=np.unique(train_data.classes), y=train_data.classes)
class_weights = dict(enumerate(class_weights))

# Define callbacks
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)

# Use test_data for validation
val_data = test_data

cnn_history = cnn_model.fit(
    train_data,
    validation_data=val_data,
    epochs=30,
    class_weight=class_weights,
    callbacks=[early_stop, reduce_lr]
)

"""âš–ï¸ Interpretation:


Improves CNN training using class balancing and callbacks ğŸ§ ğŸ“ˆ:

âš™ï¸ compute_class_weight â†’ Handles class imbalance by giving more weight to underrepresented emotions.

ğŸ§© EarlyStopping â†’ Stops training early if validation loss doesnâ€™t improve (prevents overfitting).

ğŸ” ReduceLROnPlateau â†’ Lowers the learning rate when progress slows for smoother convergence.

ğŸš€ Trains the CNN for up to 30 epochs, using balanced class weights and validation data for better accuracy and stability.
"""

train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

"""ğŸŒ€ Interpretation:


Creates an image data generator for training that performs data augmentation to boost model generalization ğŸ“ˆ:

ğŸ”„ Rescales pixel values (0â€“1).

ğŸ¯ Applies random rotations, shifts, shear, zoom, and flips.

ğŸ§© Uses fill_mode='nearest' to fill in missing pixels after transformations.
ğŸ‘‰ Helps the model learn from more diverse and realistic variations of the training images.
"""

feature_extractor = tf.keras.Model(inputs=cnn_model.input, outputs=cnn_model.layers[-2].output)
features = feature_extractor.predict(train_data)

"""ğŸ§  Interpretation:


Creates a feature extractor from the trained CNN to capture learned image representations ğŸ¯:

ğŸ” feature_extractor â†’ Outputs the second-last layer (deep features, not final predictions).

ğŸ“Š predict(train_data) â†’ Generates feature vectors for all training images, which can be used for visualization, clustering, or training traditional ML models (like SVM or Random Forest).
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_feat)
X_test_scaled = scaler.transform(X_test_feat)

"""âœ… Explanation:


This code standardizes (normalizes) your extracted CNN features before passing them to traditional ML models like SVM or Random Forest.
"""

import cv2

def crop_face(image_path):
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)

    for (x,y,w,h) in faces:
        face = img[y:y+h, x:x+w]
        return face
    return img  # if no face detected

# Example:
face = crop_face("happy1.jpeg")
cv2.imwrite("cropped_face.jpeg", face)

"""âœ… Explanation:

This function automatically detects and crops a face region from an image using OpenCVâ€™s Haar Cascade classifier.
"""

# ===========================
# Predict on uploaded image
# ===========================
from google.colab import files
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import numpy as np
import cv2
import joblib

# If you restarted the Colab session, reload models:
# cnn_model = tf.keras.models.load_model("emotion_cnn_model.h5")
# mobilenet_model = tf.keras.models.load_model("emotion_mobilenet_model.h5")
# svm_model = joblib.load("svm_model.pkl")
# rf_model = joblib.load("rf_model.pkl")

# Upload image
uploaded = files.upload()
img_path = list(uploaded.keys())[0]
print("Uploaded:", img_path)

# class labels - ensure same order
class_labels = ['angry','disgust','fear','happy','neutral','sad','surprise']

# --- prepare for CNN (grayscale) ---
img_g = image.load_img(img_path, color_mode='grayscale', target_size=(48,48))
x_g = image.img_to_array(img_g)
x_g = np.expand_dims(x_g, axis=0) / 255.0

# --- prepare for MobileNet (RGB) ---
img_r = image.load_img(img_path, color_mode='rgb', target_size=(48,48))
x_r = image.img_to_array(img_r)
x_r = np.expand_dims(x_r, axis=0) / 255.0

# Predict - CNN
cnn_probs = cnn_model.predict(x_g)
cnn_pred_idx = np.argmax(cnn_probs, axis=1)[0]
cnn_emotion = class_labels[cnn_pred_idx]

# Predict - MobileNetV2
mob_probs = mobilenet_model.predict(x_r)
mob_pred_idx = np.argmax(mob_probs, axis=1)[0]
mob_emotion = class_labels[mob_pred_idx]

# Extract features with the same feature_extractor (ensure cnn_model called earlier)
img_feat = feature_extractor.predict(x_g)       # shape (1, feature_dim)
img_feat_flat = img_feat.reshape(1, -1)

# ML predictions (SVM / RF)
svm_pred = svm_model.predict(img_feat_flat)[0]
rf_pred  = rf_model.predict(img_feat_flat)[0]
svm_emotion = class_labels[int(svm_pred)]
rf_emotion  = class_labels[int(rf_pred)]

print("CNN:", cnn_emotion)
print("MobileNetV2:", mob_emotion)
print("SVM:", svm_emotion)
print("RandomForest:", rf_emotion)

# Display the image and labels
img_disp = cv2.imread(img_path)[:,:,::-1]
plt.figure(figsize=(5,5))
plt.imshow(img_disp)
plt.axis('off')
plt.title(f"CNN: {cnn_emotion} | MobileNet: {mob_emotion}\nSVM: {svm_emotion} | RF: {rf_emotion}")
plt.show()

"""âœ… Explanation

This code takes an uploaded image, processes it for CNN, MobileNet, and ML classifiers (SVM, Random Forest), and shows their emotion predictions.
"""

# ===========================
# Predict on uploaded image
# ===========================
from google.colab import files
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import numpy as np
import cv2
import joblib

# If you restarted the Colab session, reload models:
# cnn_model = tf.keras.models.load_model("emotion_cnn_model.h5")
# mobilenet_model = tf.keras.models.load_model("emotion_mobilenet_model.h5")
# svm_model = joblib.load("svm_model.pkl")
# rf_model = joblib.load("rf_model.pkl")

# Upload image
uploaded = files.upload()
img_path = list(uploaded.keys())[0]
print("Uploaded:", img_path)

# class labels - ensure same order
class_labels = ['angry','disgust','fear','happy','neutral','sad','surprise']

# --- prepare for CNN (grayscale) ---
img_g = image.load_img(img_path, color_mode='grayscale', target_size=(48,48))
x_g = image.img_to_array(img_g)
x_g = np.expand_dims(x_g, axis=0) / 255.0

# --- prepare for MobileNet (RGB) ---
img_r = image.load_img(img_path, color_mode='rgb', target_size=(48,48))
x_r = image.img_to_array(img_r)
x_r = np.expand_dims(x_r, axis=0) / 255.0

# Predict - CNN
cnn_probs = cnn_model.predict(x_g)
cnn_pred_idx = np.argmax(cnn_probs, axis=1)[0]
cnn_emotion = class_labels[cnn_pred_idx]

# Predict - MobileNetV2
mob_probs = mobilenet_model.predict(x_r)
mob_pred_idx = np.argmax(mob_probs, axis=1)[0]
mob_emotion = class_labels[mob_pred_idx]

# Extract features with the same feature_extractor (ensure cnn_model called earlier)
img_feat = feature_extractor.predict(x_g)       # shape (1, feature_dim)
img_feat_flat = img_feat.reshape(1, -1)

# ML predictions (SVM / RF)
svm_pred = svm_model.predict(img_feat_flat)[0]
rf_pred  = rf_model.predict(img_feat_flat)[0]
svm_emotion = class_labels[int(svm_pred)]
rf_emotion  = class_labels[int(rf_pred)]

print("CNN:", cnn_emotion)
print("MobileNetV2:", mob_emotion)
print("SVM:", svm_emotion)
print("RandomForest:", rf_emotion)

# Display the image and labels
img_disp = cv2.imread(img_path)[:,:,::-1]
plt.figure(figsize=(5,5))
plt.imshow(img_disp)
plt.axis('off')
plt.title(f"CNN: {cnn_emotion} | MobileNet: {mob_emotion}\nSVM: {svm_emotion} | RF: {rf_emotion}")
plt.show()

"""âœ… Explanation

This code takes an uploaded image, processes it for CNN, MobileNet, and ML classifiers (SVM, Random Forest), and shows their emotion predictions.
"""

# Assuming test_gen and test_gen_rgb generators are already defined
# Assuming cnn_model and mobilenet_model are already trained

# Placeholder histories in case training cells were not run
try:
    cnn_history
except NameError:
    class MockHistory:
        def __init__(self):
            self.history = {'accuracy': [0], 'val_accuracy': [0], 'loss': [0], 'val_loss': [0]}
    cnn_history = MockHistory()

try:
    mobilenet_history
except NameError:
    class MockHistory:
        def __init__(self):
            self.history = {'accuracy': [0], 'val_accuracy': [0], 'loss': [0], 'val_loss': [0]}
    mobilenet_history = MockHistory()


cnn_eval = cnn_model.evaluate(test_gen)
mobilenet_eval = mobilenet_model.evaluate(test_gen_rgb)

print(f"\nCustom CNN - Accuracy: {cnn_eval[1]:.4f}, Loss: {cnn_eval[0]:.4f}")
print(f"MobileNetV2 - Accuracy: {mobilenet_eval[1]:.4f}, Loss: {mobilenet_eval[0]:.4f}")

# Visualization
plt.plot(cnn_history.history['accuracy'], label='CNN Train Acc')
plt.plot(cnn_history.history['val_accuracy'], label='CNN Val Acc')
plt.plot(mobilenet_history.history['accuracy'], label='MobileNet Train Acc')
plt.plot(mobilenet_history.history['val_accuracy'], label='MobileNet Val Acc')
plt.title('Model Accuracy Comparison')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""âœ… Explanation

This section evaluates your trained CNN and MobileNetV2 models on the test dataset and visualizes their training accuracy trends.
"""

# --- LOSS COMPARISON ---
plt.figure(figsize=(8,6))
plt.plot(cnn_history.history['loss'], label='CNN Train Loss')
plt.plot(cnn_history.history['val_loss'], label='CNN Val Loss')
plt.plot(mobilenet_history.history['loss'], label='MobileNet Train Loss')
plt.plot(mobilenet_history.history['val_loss'], label='MobileNet Val Loss')
plt.title('Model Loss Comparison')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""âœ… Explanation

This section visualizes and compares the training and validation loss of your Custom CNN and MobileNetV2 models.
"""

# Assuming test_gen_rgb and mobilenet_model are defined

# Get predictions from MobileNetV2 model
Y_pred_mobilenet = mobilenet_model.predict(test_gen_rgb)
y_pred_mobilenet = np.argmax(Y_pred_mobilenet, axis=1)

# Confusion Matrix for MobileNetV2
print('Confusion Matrix for MobileNetV2:\n')
cm_mobilenet = confusion_matrix(test_gen_rgb.classes, y_pred_mobilenet)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_mobilenet, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix - MobileNetV2')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""âœ… Explanation

This section evaluates your MobileNetV2 modelâ€™s classification performance on the test dataset by creating a confusion matrix.
"""

# Assuming test_gen and cnn_model are defined

# Get predictions from CNN model
Y_pred_cnn = cnn_model.predict(test_gen)
y_pred_cnn = np.argmax(Y_pred_cnn, axis=1)

# Confusion Matrix for CNN
print('Confusion Matrix for Custom CNN:\n')
cm_cnn = confusion_matrix(test_gen.classes, y_pred_cnn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix - Custom CNN')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""âœ… Explanation

This code evaluates how well your custom CNN model classifies each emotion in the FER2013 dataset, just like MobileNetV2.
"""

import pandas as pd
from sklearn.metrics import f1_score, accuracy_score

# Assuming report_df_cnn and report_df_mobilenet are available from previous steps
# Assuming svm_model and rf_model are trained and X_test_feat and y_test are available

# Extract overall accuracy and weighted avg F1-score from classification reports
cnn_accuracy = report_df_cnn.loc['accuracy', 'f1-score'] # Accuracy is in the f1-score column for the 'accuracy' row
cnn_f1_weighted = report_df_cnn.loc['weighted avg', 'f1-score']

mobilenet_accuracy = report_df_mobilenet.loc['accuracy', 'f1-score'] # Accuracy is in the f1-score column for the 'accuracy' row
mobilenet_f1_weighted = report_df_mobilenet.loc['weighted avg', 'f1-score']

# Get predictions from SVM and Random Forest models
svm_pred = svm_model.predict(X_test_feat)
rf_pred = rf_model.predict(X_test_feat)

# Calculate accuracy for SVM and Random Forest (if not already calculated)
svm_acc = accuracy_score(y_test, svm_pred)
rf_acc = accuracy_score(y_test, rf_pred)

# Calculate weighted F1 for SVM and RF
svm_f1_weighted = f1_score(y_test, svm_pred, average='weighted')
rf_f1_weighted = f1_score(y_test, rf_pred, average='weighted')


# Create a comparison table
data = {
    'Model': ['Custom CNN', 'MobileNetV2', 'SVM (CNN Features)', 'Random Forest (CNN Features)'],
    'Accuracy': [cnn_accuracy, mobilenet_accuracy, svm_acc, rf_acc],
    'Weighted Avg F1-Score': [cnn_f1_weighted, mobilenet_f1_weighted, svm_f1_weighted, rf_f1_weighted]
}

comparison_df = pd.DataFrame(data)

print("Model Performance Comparison:")
display(comparison_df)

"""âœ… Explanation

This section builds a performance summary table ğŸ“Š comparing all four emotion recognition models â€” CNN, MobileNetV2, SVM, and Random Forest â€” using accuracy and weighted F1-scores.
"""

# Save the trained models
cnn_model.save("emotion_cnn_model.keras")
mobilenet_model.save("emotion_mobilenet_model.keras")

# Save the scikit-learn models using joblib
import joblib
joblib.dump(svm_model, "svm_model.pkl")
joblib.dump(rf_model, "rf_model.pkl")

print("Models saved successfully!")

"""This code block safely saves all your trained models for reuse later without retraining.

# **Conclusion:**

This notebook explored different approaches for facial emotion detection using the FER2013 dataset, including a custom CNN, transfer learning with MobileNetV2, and traditional ML models (SVM and Random Forest) trained on CNN-extracted features.

The results show that the deep learning models, particularly the Custom CNN, achieved better overall performance compared to the traditional ML models on this dataset, based on accuracy and weighted average F1-score. However, the performance can vary for individual emotion classes, as seen in the confusion matrices and per-class F1-score visualizations.

Further improvements could involve hyperparameter tuning for all models, exploring different data augmentation techniques, using more advanced pre-trained models, or implementing ensemble methods. This project provides a solid foundation for building more robust facial emotion recognition systems.
"""